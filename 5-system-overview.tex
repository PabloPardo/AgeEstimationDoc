\chapter{Method} \label{chap:method}
In this work tow methods combining state of the art techniques have been proposed. A full description is delineated in this Chapter.

\section{Preprocessing}

Given that the background information is not needed in any of the two proposed methods, it is required to first detect the faces from the images and second align all of them (Figure \ref{fig:preprocessing}).

\textbf{Face detection} was performed using the Viola Jones algorithm from the open source library OpenCV. A Haar Cascade classifier was trained with nearly 5000 face images and more than 9500 negative samples. After detecting the face, the image is cropped and resized into a $200 \times 200$.

\textbf{Face alignment}, 68 facial landmarks were extracted for every detected face with a version of the algorithm proposed by Shaoqing Ren et al. \cite{ren2014face}. The algorithm was trained with almost 5000 face labelled images from FG-NET, AFW, HELEN, IBUG and LFPW face databases. After extracting all the landmarks all the faces are aligned.

All the images where transformed into \textbf{grey scale} because of the variation of the data in terms of colour (some images are in grey scale other in RGB).

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{figures/preprocessing}
	\caption{Face Detection and Face Alignment}
	\label{fig:preprocessing}
\end{figure}


\section{Biologically Inspired Method}
As mentioned in Section \ref{subsec:BIF} Biologically Inspired Features (BIF) have been proven to work specially good in age estimation \cite{conf/cvpr/GuoMFH09}\cite{han:age}. In this work a method is proposed using BIF as age representation and a hierarchical framework with SVM and SVR as a learning method.
 
\subsection{Background}

Inspired by how the visual cortex works, Maximilian Riesenhuber and Tomaso Poggio \cite{Riesenhuber99hierarchicalmodels} proposed the BIF model for object recognition called ``HMAX'' based on the hierarchical model of the visual nervous system proposed by D. H. Hubel and T. N. Wiesel \cite{Hubel:62}. 

The ``HMAX'' model is composed by layers that will contain increasingly sophisticated representations. There are two type of layers called simple ($S_1$) and complex ($C_1$). Layer $S_1$ consists a battery of Gabor filters with different orientation and scales and layer $C_1$ is a pooling layer, in this model is a ``MAX'' pooling layer. There are many variations and extensions of this model made by many authors, Serre et al. \cite{4069258}\cite{1467551} introducing two more layers $S_2$ and $C_2$ or Mayers and Wolf \cite{Meyers:2008:UBI:1325290.1325298} uses a spacial $S_2$ layer called $S_2$ facial features (S2FF) for face recognition.

\begin{table}[!h]
	\centering
	\begin{tabular}{|M{2cm}|M{2cm}|M{1.5cm}|c|c|c|}
		\hline
		\multicolumn{3}{|c|}{$C_1$ Layer}                                               & \multicolumn{3}{c|}{$S_1$ Layer}                \\ \hline
		Scale Band $S$    & Pool grid $(N_S\times N_S)$    & Overlap $\Delta s$     & Filter size $s$  & Gabor $\sigma$ & Gabor $\lambda$ \\ \hline
		\multirow{2}{*}{Band 1} & \multirow{2}{*}{$6 \times 6$}   & \multirow{2}{*}{3}  & $5 \times 5$   & 2.0            & 2.5           \\
		&                                 &                     & $7 \times 7$   & 2.8            & 3.5           \\ \hline
		\multirow{2}{*}{Band 2} & \multirow{2}{*}{$8 \times 8$}   & \multirow{2}{*}{4}  & $9 \times 9$   & 3.6            & 4.6           \\
		&                                 &                     & $11 \times 11$ & 4.5            & 5.6           \\ \hline
		\multirow{2}{*}{Band 3} & \multirow{2}{*}{$10 \times 10$} & \multirow{2}{*}{5}  & $13 \times 13$ & 5.4            & 6.8           \\
		&                                 &                     & $15 \times 15$ & 6.3            & 7.9           \\ \hline
		\multirow{2}{*}{Band 4} & \multirow{2}{*}{$12 \times 12$} & \multirow{2}{*}{6}  & $17 \times 17$ & 7.3            & 9.1           \\
		&                                 &                     & $19 \times 19$ & 8.2            & 10.3          \\ \hline
		\multirow{2}{*}{Band 5} & \multirow{2}{*}{$14 \times 14$} & \multirow{2}{*}{7}  & $21 \times 21$ & 9.2            & 11.5          \\
		&                                 &                     & $23 \times 23$ & 10.2           & 12.7          \\ \hline
		\multirow{2}{*}{Band 6} & \multirow{2}{*}{$16 \times 16$} & \multirow{2}{*}{8}  & $25 \times 25$ & 11.3           & 14.1          \\
		&                                 &                     & $27 \times 27$ & 12.3           & 15.4          \\ \hline
		\multirow{2}{*}{Band 7} & \multirow{2}{*}{$18 \times 18$} & \multirow{2}{*}{9}  & $29 \times 29$ & 13.4           & 16.8          \\
		&                                 &                     & $31 \times 31$ & 14.6           & 18.2          \\ \hline
		\multirow{2}{*}{Band 8} & \multirow{2}{*}{$20 \times 20$} & \multirow{2}{*}{10} & $33 \times 33$ & 15.8           & 19.7          \\
		&                                 &                     & $35 \times 35$ & 17.0           & 21.2          \\ \hline
	\end{tabular}
	\caption{$S_1$ and $C_1$ parameters.}
	\label{tab:bif_param}
\end{table}

The proposed method in this work used the BIF model described by Guo et al. \cite{conf/cvpr/GuoMFH09} that uses ``STD'' pool operator instead of ``MAX'' operator (Figure \ref{fig:bif}). 

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{figures/BIF}
	\caption{Biologically Inspired Features (BIF)}
	\label{fig:bif}
\end{figure}


\textbf{$S_1$ layer} is formed by $S_1$ units which take as input a grey image $I$ of $200\times200$ pixels. These units are usually modelled by Gabor filters,

\begin{equation}
G(x,y) = exp(-\frac{X^2+\gamma^2 Y^2}{2\sigma^2})\cdot cos(\frac{2\pi}{\lambda}X)
\end{equation}

where $X=x\cos\theta + y\sin\theta$ and $Y = -x\sin\theta + y\cos\theta$ are the rotations of the filter with angle $\theta\in[0,\pi]$. The aspect ration $\gamma$ is set to $0.3$, the width $\sigma$, the wavelength $\lambda$ and the filter sizes $s$ are adjusted as in Table \ref{tab:bif_param}. These parameters are empirically determined based on reactions of the visual cortex to real stimuli \cite{4069258}. 

\textbf{$C_1$ layer} is formed by $C_1$ units which pool over the $S_1$ units with the same orientation and from the same scale band (see Table \ref{tab:bif_param}). Each scale band $S$ contains two adjacent filter sizes, for instance, scale band 1 contains filter with sizes $5\times5$ and $7\times7$. The scale band also determines the sizes of the neighbourhood over which the $C_1$ units pool ($N_S\times N_S$). The pooling operator used in this method is the ``STD'' operator proposed by Guo et al. \cite{conf/cvpr/GuoMFH09},

\begin{equation}
std_{j,j+1} = \sqrt{\frac{1}{N_S\times N_S}\sum_{i=1}^{N_S\times N_S}(F_i - \bar{F})^2} 
\end{equation}

where $F_i$ is the maximum value of two consecutive $S_1$ units output in the same scale band at pixel index $i$, 

\begin{equation}
F_i = \max(x_i^j,x_i^{j+1})
\end{equation}

where $x_i^j$ and $x_i^{j+1}$ are the filtered values with scales $j$ and $j+1$ at position $i$. $\bar{F}$ is the mean of the filtered values in the neighbourhood $N_S\times N_S$.

The $C_1$ units are concatenated into a single feature vector for each input image.


\subsection{System overview}

This method combines a hierarchical framework and a hybrid face model, mixing BIF features and face shape landmarks to optimize the performance. The pipeline consists of three steps (see Figure \ref{fig:pipeline}).

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{figures/pipeline}
	\caption{Biologically Inspired Method Pipeline}
	\label{fig:pipeline}
\end{figure}

\textbf{Feature extraction}: 
After the face alignment is done, BIF features are extracted from each face image. Because of the high dimensionality of the data ($\sim3000$ BIF features and 138 shape landmark coordinates) feature reduction is needed. Principal Component Analysis (PCA) is used to reduce dimensionality of both shape and BIF. As shows Figure \ref{fig:pipeline} BIF and shape features are concatenated into a single feature vector.

\textbf{Age Group Classification}:
In this stage the images are classified into three age groups with age ranges (0-18), (19-45) and (45-100). The classifier used is a linear SVM as suggested in \cite{4531189}.

\textbf{Age Estimation}:
Three SVR with RBF kernel were trained to perform age regression in a specific age range. The data used to fit the regressors were images within 5-years-overlap between age ranges to reduce the misclassification error as proposed in \cite{han:age}. 

In order to improve the system performance a last fusion step is done by averaging the two nearest age regressors, i.e. if the age group classifier predicts that a given face belongs to the adult age group (19-45)] and the adult-specific regression function predicts that the exact age is 43, then in the fusion step would average the adult regressor with the elder regressor.


\section{Deep Learning Method}
\subsection{Background}
\subsection{System overview}